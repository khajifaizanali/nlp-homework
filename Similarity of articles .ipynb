{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "with open ('docs/doc-1.txt') as f:\n",
    "    sen_token = sent_tokenize(f.read())\n",
    "with open ('docs/doc-2.txt') as f2:\n",
    "    sen_token2 = sent_tokenize(f2.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "sentences=[]\n",
    "for i in sen_token:\n",
    "        example1 = [lemma.lemmatize(token) for token in word_tokenize(i)]\n",
    "        sentences.append(\" \".join(example1))\n",
    "sentences2=[]\n",
    "for i in sen_token2:\n",
    "        example1 = [lemma.lemmatize(token) for token in word_tokenize(i)]\n",
    "        sentences2.append(\" \".join(example1))    \n",
    "text1=\" \".join(sentences)\n",
    "text2=\" \".join(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19', '26', '45', '50', '800', 'abound', 'about', 'access', 'across', 'ad', 'admitted', 'after', 'alive', 'almost', 'an', 'and', 'anticipated', 'any', 'are', 'area', 'around', 'asthma', 'at', 'attack', 'away', 'barrier', 'be', 'because', 'bed', 'been', 'body', 'both', 'breath', 'broadband', 'buoyed', 'but', 'by', 'can', 'care', 'cause', 'cell', 'center', 'chronic', 'clinic', 'come', 'condition', 'contagious', 'corn', 'coronavirus', 'cotton', 'coughing', 'country', 'couple', 'covid', 'created', 'day', 'delivering', 'despite', 'determines', 'developed', 'diabetes', 'diagnosing', 'diagnostic', 'diarrhea', 'didn', 'different', 'direction', 'distance', 'do', 'down', 'drive', 'emergency', 'endemic', 'enough', 'even', 'eventually', 'ever', 'exposed', 'false', 'far', 'fatigue', 'fearful', 'feel', 'felt', 'fever', 'field', 'figuring', 'filled', 'financial', 'fiona', 'first', 'focus', 'for', 'forgetting', 'fragment', 'from', 'germ', 'getting', 'go', 'got', 'great', 'ha', 'had', 'have', 'having', 'headache', 'health', 'heart', 'her', 'here', 'highlighted', 'hit', 'hive', 'hoc', 'home', 'hospital', 'how', 'hurdle', 'hypertension', 'if', 'implementing', 'important', 'improved', 'in', 'increasingly', 'infected', 'initial', 'inside', 'intense', 'internet', 'is', 'isolation', 'issue', 'it', 'just', 'kick', 'kind', 'knowledge', 'known', 'labeled', 'land', 'length', 'like', 'limitation', 'll', 'long', 'lost', 'lowenstein', 'major', 'manage', 'managing', 'mantra', 'many', 'march', 'may', 'meant', 'medical', 'melanoma', 'middle', 'mile', 'minor', 'mississippi', 'month', 'monthly', 'most', 'must', 'need', 'next', 'non', 'north', 'not', 'now', 'number', 'of', 'old', 'on', 'open', 'or', 'ordinarily', 'ordinary', 'organization', 'out', 'over', 'oxygen', 'pandemic', 'part', 'past', 'patchwork', 'patient', 'pattern', 'percent', 'perfectly', 'persistence', 'place', 'plagued', 'playing', 'population', 'positive', 'possible', 'present', 'primary', 'put', 'ranging', 'rash', 'rather', 're', 'reach', 'receive', 'reception', 'regulation', 'relapses', 'remote', 'researchers', 'response', 'revolution', 'ruleville', 'rural', 'say', 'scientists', 'scrambling', 'see', 'sense', 'service', 'severe', 'she', 'shed', 'short', 'should', 'showing', 'sick', 'since', 'situation', 'smaller', 'smell', 'so', 'some', 'someone', 'sometimes', 'soon', 'sore', 'sound', 'soy', 'speaking', 'spiked', 'started', 'stay', 'staying', 'stick', 'still', 'stop', 'stopped', 'struggled', 'struggling', 'success', 'such', 'suffering', 'suited', 'sunflower', 'supplemental', 'survive', 'symptom', 'system', 'technology', 'telemedicine', 'term', 'test', 'tested', 'than', 'that', 'the', 'their', 'then', 'theory', 'there', 'these', 'they', 'this', 'throat', 'through', 'time', 'tissue', 'to', 'tool', 'travel', 'traveler', 'traveling', 'trouble', 'troubling', 'truck', 'trying', 'two', 'understand', 'unlucky', 'unreliable', 'up', 'urban', 'usher', 'viral', 'virtual', 'visitor', 'wa', 'week', 'weekend', 'well', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whose', 'why', 'will', 'with', 'won', 'word', 'work', 'world', 'would', 'year', 'yet']\n",
      "[[ 4  1  0  0  0  0  1  0  0  0  1  3  1  0  1  8  0  0  4  0  1  0  1  0\n",
      "   0  0  2  1  1  1  1  0  1  0  0  1  2  2  0  1  0  0  0  0  0  0  1  0\n",
      "   1  0  1  0  0  4  0  4  0  0  1  1  0  0  1  1  1  1  0  0  1  0  0  0\n",
      "   0  1  1  0  0  0  1  0  1  0  1  1  1  0  1  0  0  1  1  1  2  1  1  1\n",
      "   1  1  1  1  0  0  3  1  1  1  0  0  4  0  0  1  1  0  1  1  3  0  0  0\n",
      "   0  1  1  3  0  1  1  1  2  0  1  1  0  4  1  0  1  1  1  0  0  1  3  0\n",
      "   0  4  1  4  0  1  0  0  0  0  1  1  0  0  1  0  0  0  2  0  1  0  0  1\n",
      "   0  0  1  1  0  7  1  2  0  1  0  0  0  2  1  1  0  0  0  0  3  0  0  0\n",
      "   2  0  1  0  0  2  2  0  0  1  0  0  0  1  0  0  0  0  1  0  1  0  0  0\n",
      "   0  2  1  1  0  1  0  1  9  1  1  1  0  1  0  1  0  1  1  2  2  2  1  1\n",
      "   0  0  2  1  3  2  0  1  1  1  0  1  1  0  0  0  0  0  1  0  2  0  0  0\n",
      "   1  1  1  0  3  9  1  2  0  1  1  1  1  1  0  1  1  8  0  0  0  0  1  1\n",
      "   1  1  1  2  1  0  0  0  0  2  0  0  4  1  1  0  1  0  1  1  1  0  1  0\n",
      "   1  0  0  0  1  1  0  1  1  0]\n",
      " [ 2  0  1  1  1  1  0  1  1  1  0  0  0  1  0  9  1  1  1  1  0  1  3  1\n",
      "   1  1  0  0  0  2  0  1  0  1  1  1  1  0  3  0  1  3  1  1  2  1  0  1\n",
      "   1  1  0  2  1  2  1  0  1  1  0  0  1  1  0  0  0  0  1  1  0  1  1  1\n",
      "   1  0  0  1  1  1  0  1  0  1  0  0  0  1  0  1  1  0  1  0  3  0  0  3\n",
      "   0  0  0  0  1  2  0  4  0  0  3  1  0  1  1  0  0  1  2  5  0  1  1  1\n",
      "   1  0  0  5  1  0  0  0  0  1  2  0  1  2  0  1  0  0  0  1  1  0  0  1\n",
      "   1  1  0  0  1  0  1  1  1  1  0  0  3  1  0  2  1  1  1  1  0  1  3  0\n",
      "   2  2  1  0  1  9  0  0  1  0  1  1  1  1  0  0  3  1  1  1  5  1  1  1\n",
      "   0  1  1  1  2  0  0  1  1  0  1  1  1  0  1  1  1  1  0  1  0  1  1  1\n",
      "   3  0  0  0  1  0  1  0  0  0  0  0  1  0  1  0  1  0  0  2  0  0  0  0\n",
      "   1  1  0  0  0  0  1  0  0  0  1  0  0  1  1  1  1  2  0  1  0  2  1  2\n",
      "   0  0  0  2  0 16  0  0  1  0  2  1  1  0  2  0  0  9  1  1  1  1  0  0\n",
      "   0  0  0  0  0  1  1  1  1  0  2  1  0  0  0  1  1  1  0  0  0  1  0  1\n",
      "   0  2  1  1  0  1  1  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([text1,text2])\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
